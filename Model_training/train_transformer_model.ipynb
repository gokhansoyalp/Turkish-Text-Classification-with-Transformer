{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDrKa7AGI2RQ"
      },
      "outputs": [],
      "source": [
        "#check gpu running\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO3nLDFYJa8G"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si8trCJYJc9x"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPa2jaM8Jet6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"cwd \", os.getcwd())\n",
        "print(\"ls \", os.listdir())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koVhsY6oJhXK"
      },
      "outputs": [],
      "source": [
        "path=\"/content/gdrive/My Drive/\"\n",
        "print(\"ls \", os.listdir(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_lc5kjNJjzk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zlhxy85Jlxi"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "df = pd.read_csv(path+'file_name.csv', encoding='utf-8-sig')\n",
        "df.head(-1)\n",
        "#df.hist(bins=20, grid=False, figsize=(10,6), zorder=2 )\n",
        "\n",
        "#ax = df.hist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIwqFDLhJnxp"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53RaJUSFJpw8"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "review_lines = list()\n",
        "lines = df['Description'].values.tolist()\n",
        "\n",
        "#lines.pop(18060)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwnzi35fJr-5"
      },
      "outputs": [],
      "source": [
        "for line in lines:\n",
        "\n",
        "    tokens = word_tokenize(line)\n",
        "    # convert to lower case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # remove punctuation from each word    \n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('turkish'))\n",
        "    #print(stop_words)\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    review_lines.append(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw_i_bZJJuXc"
      },
      "outputs": [],
      "source": [
        "import gensim \n",
        "\n",
        "EMBEDDING_DIM = 200\n",
        "# train word2vec model\n",
        "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=20, workers=5, min_count=30,negative=5,iter=20)\n",
        "# vocab size\n",
        "words = list(model.wv.vocab)\n",
        "print('Vocabulary size: %d' % len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo7-oj3PJ0aj"
      },
      "outputs": [],
      "source": [
        "# save model in ASCII (word2vec) format\n",
        "filename = path+'deneme.txt'\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z1oL7cLJ4N0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('', path+'200_egitim.txt'),  encoding = \"utf-8-sig\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LtrT4DAJ96B"
      },
      "outputs": [],
      "source": [
        "max_length =200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yy_YU5PKDQ6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer_obj = Tokenizer(num_words=50000)\n",
        "#tokenizer_obj.num_words=50000 #50000 most frequent words will be kept\n",
        "tokenizer_obj.fit_on_texts(review_lines)\n",
        "#print (\"tokenizer_obj.word_count \",tokenizer_obj.word_counts)\n",
        "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
        "\n",
        "# pad sequences\n",
        "word_index = tokenizer_obj.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
        "sentiment =  df['Category'].values\n",
        "print('Shape of review tensor:', review_pad.shape)\n",
        "print('Shape of sentiment tensor:', sentiment.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "indices = np.arange(review_pad.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "review_pad = review_pad[indices]\n",
        "sentiment = sentiment[indices]\n",
        "\n",
        "X_train=review_pad[:17786]\n",
        "y_train=sentiment[:17786]\n",
        "X_test=review_pad[17786:]\n",
        "y_test=sentiment[17786:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2xF9JKZKIL9"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 200\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6jIRQUAKMnq"
      },
      "outputs": [],
      "source": [
        "#y_train=np.where(y_train == 5, 0 , y_train)\n",
        "y_train=tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
        "#y_test=np.where(y_test == 5, 0 , y_test)\n",
        "y_test=tf.keras.utils.to_categorical(y_test, num_classes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmkE7k9bKReu"
      },
      "outputs": [],
      "source": [
        "#test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
        "#test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "#predict\n",
        "#model.predict(x=test_samples_tokens_pad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSvnhyXMLd0x"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YzFK-YaLiNh"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'query_dense': self.query_dense,\n",
        "            'key_dense': self.key_dense,\n",
        "            'value_dense': self.value_dense,\n",
        "            'combine_heads': self.combine_heads,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DONnqdKsLnlF"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"softmax\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'att': self.att,\n",
        "            'ffn': self.ffn,\n",
        "            'layernorm1': self.layernorm1,\n",
        "            'layernorm2': self.layernorm2,\n",
        "            'dropout1': self.dropout1,\n",
        "            'dropout2': self.dropout2,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQWTU2t-LoiE"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XupA4_rMId-"
      },
      "outputs": [],
      "source": [
        "num_heads = 5  # Number of attention heads\n",
        "max_length=200\n",
        "ff_dim = 200  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=(max_length,))\n",
        "\n",
        "#embedding_layer = TokenAndPositionEmbedding(max_length, num_words, EMBEDDING_DIM)\n",
        "\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=max_length,\n",
        "                            trainable=False)\n",
        "\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(EMBEDDING_DIM, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(512, activation=\"softmax\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(256, activation=\"softmax\")(x)\n",
        "\n",
        "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41yFTc17MKmR"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_dxIAJ1MLdq"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ukmd2QMNKl"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, batch_size=512, epochs=10, validation_data=(X_test, y_test),verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0IoO7q6U-nK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.argmax(y_pred,axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4G7VwqA5tOI"
      },
      "outputs": [],
      "source": [
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSu9vZj25u81"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(path+'model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reYCdtHjhMX3"
      },
      "outputs": [],
      "source": [
        "model = model.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3g7Hszs6Ff6"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGrUDJ1whefj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8WKDmghEaqd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkWsIaCcMRFD"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict_classes(X_test)\n",
        "con_mat = tf.math.confusion_matrix(labels=y_test.argmax(axis=1), predictions=X_test).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ue3DSSNMTfN"
      },
      "outputs": [],
      "source": [
        "classes=[0,1,2,3,4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikMfyniIMVDV"
      },
      "outputs": [],
      "source": [
        "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                     index = classes, \n",
        "                     columns = classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu7IzyQSMWoc"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}